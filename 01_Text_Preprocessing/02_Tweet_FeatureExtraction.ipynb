{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c48c81-33c8-4cb0-9acf-3c7de8194d36",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Machine Learning for NLP</h1>\n",
    "    <h2 align=\"center\">Text Feature Extraction</h2>\n",
    "    <h3 align=\"center\">Zahra Amini</h3>\n",
    "<div style=\"width: 100%; text-align: center;\">\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a class=\"link\" href=\"https://t.me/Zahraamini_ai\">Telegram</a><br>\n",
    "                <a class=\"link\" href=\"https://www.linkedin.com/in/zahraamini-ai/\">LinkedIn</a><br>\n",
    "                <a class=\"link\" href=\"https://www.youtube.com/@AcademyHobot\">YouTube</a><br>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a class=\"link\" href=\"https://github.com/aminizahra\">GitHub</a><br>\n",
    "                <a class=\"link\" href=\"https://www.kaggle.com/aminizahra\">Kaggle</a><br>\n",
    "                <a class=\"link\" href=\"https://www.instagram.com/zahraamini_ai/\">Instagram</a><br>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d4efc-4b21-4f6d-9b34-d97ae9ecf05a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf7ab05-be67-40a6-980c-8ea5e40f04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb5e0e9b-152d-49ed-8a55-546788aa82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b4989d7-3474-4ed3-938e-b9ae7d5b8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fb71b-6f08-4f02-bea3-16cc7b7afe1a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2103b0d0-2a1c-4c87-8bd4-21f834b7b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'preprocessedTweet.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63392fb-6c1a-44d4-8c3e-70351f71cbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>switchfoot   awww thats a bummer  you shoulda ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['switchfoot   awww thats a bummer  you should...</td>\n",
       "      <td>['switchfoot', 'awww', 'thats', 'a', 'bummer',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['is upset that he cant update his facebook by...</td>\n",
       "      <td>['is', 'upset', 'that', 'he', 'cant', 'update'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['kenichan i dived many times for the ball man...</td>\n",
       "      <td>['kenichan', 'i', 'dived', 'many', 'times', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>negative</td>\n",
       "      <td>['my whole body feels itchy and like its on fi...</td>\n",
       "      <td>['my', 'whole', 'body', 'feels', 'itchy', 'and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['nationwideclass no its not behaving at all i...</td>\n",
       "      <td>['nationwideclass', 'no', 'its', 'not', 'behav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text sentiment  \\\n",
       "0       0  switchfoot   awww thats a bummer  you shoulda ...  negative   \n",
       "1       0  is upset that he cant update his facebook by t...  negative   \n",
       "2       0  kenichan i dived many times for the ball manag...  negative   \n",
       "3       0    my whole body feels itchy and like its on fire   negative   \n",
       "4       0  nationwideclass no its not behaving at all im ...  negative   \n",
       "\n",
       "                                     sentence_tokens  \\\n",
       "0  ['switchfoot   awww thats a bummer  you should...   \n",
       "1  ['is upset that he cant update his facebook by...   \n",
       "2  ['kenichan i dived many times for the ball man...   \n",
       "3  ['my whole body feels itchy and like its on fi...   \n",
       "4  ['nationwideclass no its not behaving at all i...   \n",
       "\n",
       "                                         word_tokens  \n",
       "0  ['switchfoot', 'awww', 'thats', 'a', 'bummer',...  \n",
       "1  ['is', 'upset', 'that', 'he', 'cant', 'update'...  \n",
       "2  ['kenichan', 'i', 'dived', 'many', 'times', 'f...  \n",
       "3  ['my', 'whole', 'body', 'feels', 'itchy', 'and...  \n",
       "4  ['nationwideclass', 'no', 'its', 'not', 'behav...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03842c-e04e-4446-a270-772462418df0",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\n",
    "\n",
    "The **Bag of Words (BoW)** model is a method for extracting features from text for use in machine learning algorithms. In this approach, a text is represented as the frequency of words within it, ignoring grammar and order of words. Each unique word in the text corpus is considered a feature, and its frequency count in each document forms the feature vector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe160c2-6b48-4fed-9d41-c50c345d3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f801b1c-1954-4aee-aac4-472a128133b5",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\r\n",
    "\n",
    "The** Term Frequency-Inverse Document Frequency (TF-IDF**) combines two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF). The formula for calculating TF-IDF for a word \\( t \\) in a document \\( d \\) is:\r\n",
    "\r\n",
    "$$\r\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\r\n",
    "$$\r\n",
    "\r\n",
    "Where:\r\n",
    "\r\n",
    "- **Term Frequency (TF)** measures how frequently a word appears in a document and is calculated as:\r\n",
    "\r\n",
    "$$\r\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times } t \\text{ appears in } d}{\\text{Total number of words in } d}\r\n",
    "$$\r\n",
    "\r\n",
    "- **Inverse Document Frequency (IDF)** measures how unique a word is across all documents in the corpus and is calculated as:\r\n",
    "\r\n",
    "$$\r\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing } t}\\right)\r\n",
    "$$\r\n",
    "\r\n",
    "Words with high TF-IDF scores are important because they appear frequently in a document but not across many documents in the corpus, making them unique and meaningful features.\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05de050e-5c0c-4ee3-b788-e50bb9e4775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67af601-4fa0-41db-bfc0-dee96491292b",
   "metadata": {},
   "source": [
    "## Word Embedding | Word2Vec\n",
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\n",
    "\n",
    "The **Word2Vec** model is a neural network-based method for learning vector representations of words in a continuous vector space. It captures semantic meanings by grouping similar words close to each other in the vector space. Here is an explanation of each parameter used in the code:<br>\n",
    "\n",
    "- **sentences**: This parameter represents the input data, which should be a list of tokenized sentences (each sentence is a list of words). In this example, <code>tokenized_text</code> is used as the input.<br>\n",
    "- **vector_size**: Defines the dimensionality of the word vectors. A higher value like 100 means each word is represented by a vector with 100 dimensions, capturing more details about word meanings.<br>\n",
    "- **window**: Specifies the maximum distance between the current and predicted word within a sentence. A value of 5 means the model considers up to 5 words before and after the target word, capturing more contextual information.<br>\n",
    "- **min_count**: Ignores all words with a frequency lower than this value. Setting it to 1 ensures that even words appearing once in the corpus are included in the model.<br>\n",
    "- **workers**: Sets the number of worker threads used for training. A higher number, such as 4, speeds up the training process on multi-core systems.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576963da-7232-45f3-b07a-754573f23569",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = data['word_tokens']\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe60efe-780b-4c52-9ce7-6d8703de4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence_tokens, model):\n",
    "    word_vectors = [model.wv[word] for word in sentence_tokens if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b3b8e5-e37b-44a1-8f1b-ff3994d4bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_features = [get_sentence_vector(sentence, word2vec_model) for sentence in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fcb6cd-0a01-40b7-b9f0-c6a330491d9a",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)** is a pre-trained word embedding method that captures semantic relationships between words. Unlike Word2Vec, which relies on local context windows, GloVe is trained on word co-occurrence statistics across the entire corpus. This means it considers the global word relationships, making it especially useful for capturing semantic similarities.<br><br>\n",
    "\n",
    "- **Pre-trained Embeddings**: GloVe provides pre-trained word vectors, typically trained on large datasets like Wikipedia or Common Crawl. These embeddings are loaded into the model, and words in your text are represented using these vectors.<br>\n",
    "- **Dimensionality**: GloVe embeddings come in various dimensions (e.g., 50, `100`, 200, 300). A higher dimension allows the embedding to capture more semantic detail but requires more computational resources.<br>\n",
    "- **Usage**: You can load GloVe embeddings using a library like Gensim or manually by mapping each word to its vector representation. For instance, each word in your vocabulary is represented by a pre-trained GloVe vector, which can be used in downstream tasks like text classification, clustering, or semantic analysis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d84de22-77ca-4e40-86d6-ec5af1236e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = r'C:\\PC\\Zahraamini_ai\\GitHub\\NLP_FeatureExtraction\\glove.6B\\glove.6B.100d.txt' \n",
    "# download from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01285a0a-669d-4935-8ffd-f9e57b9b788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = {}\n",
    "with open(glove_path, 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5025faee-426a-4fe4-b5cf-564781e67d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [glove_embeddings[word] for word in words if word in glove_embeddings]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b71ec9ab-7b16-4eb6-a80e-3f759633ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_features = [get_sentence_embedding(sentence) for sentence in data['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c69a08-9a08-4d4f-820d-ef402efb7ac6",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\r\n",
    "**\n",
    "Cosine similarit**y is a measure used to determine how similar two vectors are by calculating the cosine of the angle between them. It’s commonly used in text analysis to compare the similarity between two documents or sentences represented as vectors. The cosine similarity score ranges from -1 to 1, where 1 indicates identical vectors, 0 indicates orthogonal (no similarity), and -1 indicates opposite directions.<br><br>\r\n",
    "\r\n",
    "The formula for cosine similarity between two vectors \\( A \\) and \\( B \\) is:\r\n",
    "\r\n",
    "$$\r\n",
    "\\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\r\n",
    "$$\r\n",
    "\r\n",
    "Where:\r\n",
    "\r\n",
    "- \\( A \\cdot B \\) is the dot product of the vectors \\( A \\) and \\( B \\).\r\n",
    "\r\n",
    "- \\( \\|A\\| \\) and \\( \\|B\\| \\) are the magnitudes (norms) of vectors \\( A \\) and \\( B \\), calculated as:\r\n",
    "\r\n",
    "$$\r\n",
    "\\|A\\| = \\sqrt{\\sum_{i=1}^{n} A_i^2}\r\n",
    "$$\r\n",
    "\r\n",
    "$$\r\n",
    "\\|B\\| = \\sqrt{\\sum_{i=1}^{n} B_i^2}\r\n",
    "$$\r\n",
    "\r\n",
    "The result will be a number between -1 and 1, with higher values indicating greater similarity.\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c06f1-e962-4f3c-b782-192056d4d672",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\n",
    "\n",
    "In text feature extraction methods, **cosine similarity** quantifies the similarity between two text representations, typically after transforming texts into vectors using techniques like Bag of Words, TF-IDF, or Word Embeddings (e.g., Word2Vec, GloVe). By calculating the cosine of the angle between two vectors, cosine similarity reveals how semantically similar two texts are. A cosine similarity score close to 1 indicates high similarity, suggesting the texts are likely to share similar content or context.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dbbfcc9-0a0b-4cd3-91a3-dc2aa1a6312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_bow = np.mean(cosine_similarity(bow_features[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bccc8f2d-d145-4ec8-ab54-4ff5e271a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_tfidf = np.mean(cosine_similarity(tfidf_features[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d3e4e1-f1c5-44c5-9d4f-cc4ee0ef7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_word2vec = np.mean(cosine_similarity(word2vec_features[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1195867c-e79c-4a8b-806f-e0172a3d92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_glove = np.mean(cosine_similarity(glove_features[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b081348d-215c-4af2-ac57-4c249bcf81cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity for BoW:\n",
      " 0.04784706731270371\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine Similarity for BoW:\\n\", cosine_sim_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97f457-dd8b-42c6-8a63-7f493efd0ff0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\n",
    "\n",
    "This value is relatively low and indicates that the two texts have little similarity when using the Bag of Words (BoW) method. This result is often due to the fact that BoW only considers the frequency of words and does not take into account the order or **semantic relationships** of the words. Therefore, if the texts use different words, the BoW similarity **will naturally be low**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bc005df-3a9a-454d-9f88-c84b484a6f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity for TF-IDF:\n",
      " 0.01337352036602397\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine Similarity for TF-IDF:\\n\", cosine_sim_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9d7f8-acda-44af-aef0-7afa1eeb32f7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\n",
    "\n",
    "This value is even lower than that of BoW. The TF-IDF method focuses on words that hold greater significance in each text by considering the relative importance of words based on their frequency across the entire corpus. A low similarity score indicates that the key important words in these two texts differ significantly, suggesting that the texts may be thematically different or use different words altogether.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "244ff9e0-a2fb-4b03-a61a-06a36f62af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity for Word2Vec:\n",
      " 0.996087\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine Similarity for Word2Vec:\\n\", cosine_sim_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a15500-df74-46e8-b667-aa5c87896bf8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\r\n",
    "\n",
    "A high value (close to 1) indicates that the texts are semantically very similar. The Word2Vec model, by considering the semantic relationships between words, can identify similarities between words with similar meanings, even if the exact same words are not used. Therefore, this result suggests that the two texts likely share similar meanings or themes.\r\n",
    "</di>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd771069-2df8-48f0-b11a-b940209122fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity for GloVe:\n",
      " 0.8267009158948537\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine Similarity for GloVe:\\n\", cosine_sim_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31813a-eaa2-4bef-beb7-146ba4e5ca53",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\n",
    "\n",
    "This value is even lower than that of BoW. The TF-IDF method focuses on words that hold greater significance in each text by considering the relative importance of words based on their frequency across the entire corpus. A low similarity score indicates that the key important words in these two texts differ significantly, suggesting that the texts may be thematically different or use different words altogether.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d762b-867e-4427-a30d-9a78548ad201",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dafbe1; border-left: 5px solid #34c759; padding: 10px;\">\r\n",
    "\n",
    "The results indicate that Word2Vec and GloVe, both of which utilize the semantic relationships of words for feature representation, show a higher similarity between the two texts, whereas BoW and TF-IDF demonstrate a very low similarity due to their reliance on the frequency and relative importance of words. This suggests that if the goal is to measure semantic similarity between texts, using word embeddings like Word2Vec and GloVe is preferable to simpler methods like BoW and TF-IDF.\r\n",
    "</di>\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
